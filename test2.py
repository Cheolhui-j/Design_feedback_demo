import gradio as gr
import base64
import datetime

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration
from PIL import Image
import requests
from io import BytesIO

# === 1. ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ ===
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device : {device}")

model_name = "llava-hf/llava-1.5-7b-hf"  # ëª¨ë¸ëª… (ë³€ê²½ ê°€ëŠ¥)
system_prompt = "You're participating in an interview..."

# ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
processor = AutoProcessor.from_pretrained(model_name)
model = LlavaForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
)
model.to(device)

# 2-1. (ì„ íƒ ì‚¬í•­) Processor config ê°•ì œ ìˆ˜ì •
# ì¼ë¶€ ë²„ì „ì˜ LLaVA ëª¨ë¸ì—ì„  patch_size, vision_feature_select_strategyë¥¼ ë³€ê²½í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# í•„ìš” ì—†ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.
processor.patch_size = 14
processor.vision_feature_select_strategy = "default"

def load_image(image_path: str) -> Image.Image:
    """
    ì´ë¯¸ì§€ ë¡œë“œ í•¨ìˆ˜: URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ëª¨ë‘ ì²˜ë¦¬.
    """
    try:
        if image_path.startswith("http"):
            response = requests.get(image_path)
            response.raise_for_status()
            image = Image.open(BytesIO(response.content)).convert("RGB")
        else:
            image = Image.open(image_path).convert("RGB")
        return image
    except Exception as e:
        raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# === 2. ë¡œì»¬ ì¶”ë¡  í•¨ìˆ˜ ===
def generate_response(image: Image.Image, prompt: str, chat_history=None):
    # ì´ì „ ëŒ€í™” ì´ë ¥ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    context_pairs = []
    if chat_history:
        for i in range(0, len(chat_history)-1, 2):
            if i+1 < len(chat_history):
                user_msg = chat_history[i][1]
                ai_msg = chat_history[i+1][1]
                context_pairs.append(f"ì‚¬ìš©ì: {user_msg}\nAI: {ai_msg}")
    
    context = "\n".join(context_pairs)

    # 2) í”„ë¡¬í”„íŠ¸ì— <image> í† í° ì‚½ì…
    #    LLaVAëŠ” í…ìŠ¤íŠ¸ ì•ˆì— ì´ í† í°ì´ ìˆì–´ì•¼ "ì´ë¯¸ì§€ 1ì¥"ì„ ì¸ì‹í•©ë‹ˆë‹¤.
    prompt_with_image = f"<image>\n{prompt}"
    
    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    full_prompt = (
        f"{system_prompt}\n\n"
        f"ì´ì „ ëŒ€í™” ë‚´ìš©:\n{context}\n\n"
        f"ì‚¬ìš©ì: {prompt_with_image}\nAI:"
    )
    
    # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì…ë ¥ ì¤€ë¹„
    inputs = processor(
        text=full_prompt,
        images=image,
        return_tensors="pt"
    ).to(device)
    
    # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
    generation_config = {
        "max_new_tokens": 1024,
        "temperature": 0.7,
        "top_p": 0.9,
        "do_sample": True,
    }
    
    # ì¶”ë¡  ì‹¤í–‰
    with torch.inference_mode():
        output = model.generate(**inputs, **generation_config)
    
    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
    generated_text = processor.decode(output[0], skip_special_tokens=True)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì…ë ¥ ì œê±°í•˜ì—¬ ì‘ë‹µë§Œ ì¶”ì¶œ
    response = generated_text.split("AI:")[-1].strip()
    
    return response

# === 3. ì±„íŒ… ì²˜ë¦¬ í•¨ìˆ˜ ===
def chat_with_model(chat_history, image: Image.Image, user_message: str):
    if not user_message.strip():
        return chat_history, ""  # ë¹ˆ ì…ë ¥ì€ ë¬´ì‹œí•˜ê³  ì…ë ¥ì°½ ë¹„ìš°ê¸°
    
    # ì±„íŒ… ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    chat_history.append(("ì‚¬ìš©ì", user_message))
    
    if not image:
        chat_history.append(("AI", "âŒ ì´ë¯¸ì§€ë¥¼ ë°˜ë“œì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."))
        return chat_history, ""
    
    try:
        # ë¡œì»¬ ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰
        bot_response = generate_response(image, user_message, chat_history)
    except Exception as e:
        bot_response = f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}"
    
    # ì‘ë‹µ ì¶”ê°€
    chat_history.append(("AI", bot_response))
    return chat_history, ""  # ë‘ ë²ˆì§¸ ë°˜í™˜ê°’ì€ ì…ë ¥ì°½ ë¹„ìš°ê¸°

# === 4. UI êµ¬ì„± ===
with gr.Blocks(css="""
    /* ì „ì²´ ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .container {
        width: 100%;
        margin: 0 auto;
    }
    
    /* ì´ë¯¸ì§€ ì—…ë¡œë“œ ì˜ì—­ */
    .image-upload-container {
        height: 100%;
        display: flex;
        flex-direction: column;
    }
    
    /* ì´ë¯¸ì§€ ì—…ë¡œë“œ ì»´í¬ë„ŒíŠ¸ í¬ê¸° ì¡°ì • */
    .image-upload-container > div {
        height: 100%;
    }
    
    /* ì´ë¯¸ì§€ ì»¨í…Œì´ë„ˆë¥¼ ë” í¬ê²Œ */
    .image-upload-container .upload-container {
        min-height: 600px;
    }
    
    /* ì—…ë¡œë“œëœ ì´ë¯¸ì§€ê°€ ë” í¬ê²Œ ë³´ì´ë„ë¡ */
    .image-upload-container img {
        max-height: 550px !important;
        object-fit: contain;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ */
    .chat-container {
        height: 100%;
        display: flex;
        flex-direction: column;
    }
    
    /* ë§í’ì„  ìŠ¤íƒ€ì¼ */
    .chatbox {
        display: flex;
        flex-direction: column;
        gap: 8px;
        padding: 10px;
        height: 600px;  /* ë†’ì´ ì¦ê°€ */
        overflow-y: auto;
        background-color: #f5f5f5;
        border-radius: 10px;
    }
    
    .message {
        display: flex;
        max-width: 80%;
    }
    
    .user-msg {
        align-self: flex-end;
        justify-content: flex-end;
        margin-left: auto;
    }
    
    .ai-msg {
        align-self: flex-start;
        margin-right: auto;
    }
    
    .bubble {
        padding: 10px 15px;
        border-radius: 18px;
        line-height: 1.4;
        font-size: 15px;
        word-break: break-word;
    }
    
    .user-bubble {
        background-color: #FEE500; /* ì¹´ì¹´ì˜¤í†¡ ë…¸ë€ìƒ‰ */
        border-bottom-right-radius: 0;
    }
    
    .ai-bubble {
        background-color: white;
        border-bottom-left-radius: 0;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }
    
    /* ì…ë ¥ ì˜ì—­ ìŠ¤íƒ€ì¼ */
    .input-area {
        display: flex;
        gap: 8px;
        margin-top: 10px;
    }
    
    /* ì‹œê°„ í‘œì‹œ */
    .time {
        font-size: 12px;
        color: #999;
        margin-top: 2px;
        text-align: right;
    }
    
    /* ì‚¬ìš©ì ì´ë¦„ */
    .sender {
        font-size: 12px;
        color: #666;
        margin-bottom: 2px;
    }
    
    /* í”„ë¡œí•„ ì´ë¯¸ì§€ */
    .profile {
        width: 36px;
        height: 36px;
        border-radius: 50%;
        margin-right: 8px;
        background-color: #ddd;
        flex-shrink: 0;
    }
    
    /* ëŒ€í™” ê¸°ë¡ í‘œì‹œ */
    .history-indicator {
        text-align: center;
        margin: 10px 0;
        font-size: 12px;
        color: #888;
        font-style: italic;
    }
    
    /* ì´ˆê¸°í™” ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .reset-button {
        margin-top: 10px;
        text-align: center;
    }

    /* ëª¨ë¸ ì •ë³´ í‘œì‹œ */
    .model-info {
        margin-top: 15px;
        padding: 8px;
        background-color: #f0f0f0;
        border-radius: 5px;
        font-size: 13px;
        color: #555;
    }
""") as demo:
    gr.Markdown("## ğŸ’¬ ë””ìì¸ í”¼ë“œë°± ì›¹ ì•± Demo (ë¡œì»¬ LLaVa ì¶”ë¡ )")
    
    # ëª¨ë¸ ì •ë³´ í‘œì‹œ
    gr.Markdown(f"**ëª¨ë¸ ì •ë³´**: {model_name} | ì‹¤í–‰ í™˜ê²½: {device}", elem_classes="model-info")
    
    # ì¢Œìš° ë¶„í•  ë ˆì´ì•„ì›ƒ
    with gr.Row(equal_height=True):
        # ì™¼ìª½ - ì´ë¯¸ì§€ ì—…ë¡œë“œ ì˜ì—­ (ë” í° ì‚¬ì´ì¦ˆë¡œ)
        with gr.Column(scale=4, elem_classes="image-upload-container"):
            image_input = gr.Image(
                type="pil", 
                label="ë””ìì¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ (í•„ìˆ˜)", 
                interactive=True,
                height=650,  # ë†’ì´ ì¦ê°€
                elem_classes="upload-container"
            )
            
            # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
            with gr.Row(elem_classes="reset-button"):
                reset_btn = gr.Button("ëŒ€í™” ì´ˆê¸°í™”", variant="secondary")
        
        # ì˜¤ë¥¸ìª½ - ì±„íŒ… ì˜ì—­
        with gr.Column(scale=3, elem_classes="chat-container"):
            chat_state = gr.State([])
            
            # ì±„íŒ… ë©”ì‹œì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” HTML ì»´í¬ë„ŒíŠ¸
            chatbox = gr.HTML(
                value="<div class='chatbox'><div class='history-indicator'>ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ëª¨ë¸ì— ì „ë‹¬ë©ë‹ˆë‹¤</div></div>",
                elem_id="chatbox"
            )
            
            with gr.Row(elem_classes="input-area"):
                user_input = gr.Textbox(
                    placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", 
                    show_label=False,
                    elem_id="msg-input"
                )
                submit_btn = gr.Button("ì „ì†¡", variant="primary")
    
    # í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    def get_current_time():
        now = datetime.datetime.now()
        return now.strftime("%p %I:%M").lower()  # ì˜¤ì „/ì˜¤í›„ ì‹œ:ë¶„
    
    # ëŒ€í™” ì´ˆê¸°í™” í•¨ìˆ˜
    def reset_chat():
        return [], "<div class='chatbox'><div class='history-indicator'>ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ëª¨ë¸ì— ì „ë‹¬ë©ë‹ˆë‹¤</div></div>"
    
    # ëŒ€í™” ë Œë”ë§ í•¨ìˆ˜
    def render_chat(chat_history):
        messages_html = "<div class='history-indicator'>ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ëª¨ë¸ì— ì „ë‹¬ë©ë‹ˆë‹¤</div>"
        current_time = get_current_time()
        
        for i, (role, msg) in enumerate(chat_history):
            if role == "ì‚¬ìš©ì":
                messages_html += f"""
                <div class='message user-msg'>
                    <div>
                        <div class='bubble user-bubble'>{msg}</div>
                        <div class='time'>{current_time}</div>
                    </div>
                </div>
                """
            else:  # AI
                messages_html += f"""
                <div class='message ai-msg'>
                    <div class='profile'></div>
                    <div>
                        <div class='sender'>AI ë„ìš°ë¯¸</div>
                        <div class='bubble ai-bubble'>{msg}</div>
                        <div class='time'>{current_time}</div>
                    </div>
                </div>
                """
        
        return f"<div class='chatbox'>{messages_html}</div>"
    
    # ë©”ì‹œì§€ ì œì¶œ í•¨ìˆ˜
    def submit_message(chat_state, image, message):
        if not message.strip():
            return chat_state, render_chat(chat_state), ""
        
        new_history, _ = chat_with_model(chat_state, image, message)
        return new_history, render_chat(new_history), ""
    
    # ì´ë²¤íŠ¸ ì—°ê²°
    submit_btn.click(
        fn=submit_message,
        inputs=[chat_state, image_input, user_input],
        outputs=[chat_state, chatbox, user_input]
    )
    
    user_input.submit(
        fn=submit_message,
        inputs=[chat_state, image_input, user_input],
        outputs=[chat_state, chatbox, user_input]
    )
    
    reset_btn.click(
        fn=reset_chat,
        inputs=[],
        outputs=[chat_state, chatbox]
    )

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    print(f"ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {model_name}")
    print(f"ì‹¤í–‰ í™˜ê²½: {device}")
    demo.launch(share=True)

# import torch
# from transformers import AutoProcessor, LlavaForConditionalGeneration
# from PIL import Image
# import requests
# from io import BytesIO

# # 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
# device = "cuda" if torch.cuda.is_available() else "cpu"
# print(f"Using device: {device}")

# # 2. ëª¨ë¸ & í”„ë¡œì„¸ì„œ ì„¤ì •
# #   - ì›í•˜ëŠ” LLaVA ëª¨ë¸ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”. (ì˜ˆ: "llava-hf/llava-1.5-7b-hf")
# model_id = "llava-hf/llava-1.5-7b-hf"

# # Processor & Model ë¡œë“œ
# processor = AutoProcessor.from_pretrained(model_id)
# #   - device=='cuda'ì¼ ë•Œ float16, CPUì¼ ë•Œ float32
# model = LlavaForConditionalGeneration.from_pretrained(
#     model_id, 
#     torch_dtype=torch.float16 if device == "cuda" else torch.float32
# )
# model.to(device)

# # 2-1. (ì„ íƒ ì‚¬í•­) Processor config ê°•ì œ ìˆ˜ì •
# # ì¼ë¶€ ë²„ì „ì˜ LLaVA ëª¨ë¸ì—ì„  patch_size, vision_feature_select_strategyë¥¼ ë³€ê²½í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# # í•„ìš” ì—†ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.
# processor.patch_size = 14
# processor.vision_feature_select_strategy = "default"

# def load_image(image_path: str) -> Image.Image:
#     """
#     ì´ë¯¸ì§€ ë¡œë“œ í•¨ìˆ˜: URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ëª¨ë‘ ì²˜ë¦¬.
#     """
#     try:
#         if image_path.startswith("http"):
#             response = requests.get(image_path)
#             response.raise_for_status()
#             image = Image.open(BytesIO(response.content)).convert("RGB")
#         else:
#             image = Image.open(image_path).convert("RGB")
#         return image
#     except Exception as e:
#         raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# def generate_answer(image_path: str, prompt: str) -> str:
#     """
#     LLaVA ëª¨ë¸ì—ê²Œ 'ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸'ë¥¼ ì…ë ¥í•˜ì—¬ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
#     """
#     # 1) ì´ë¯¸ì§€ ë¡œë“œ
#     image = load_image(image_path)

#     # 2) í”„ë¡¬í”„íŠ¸ì— <image> í† í° ì‚½ì…
#     #    LLaVAëŠ” í…ìŠ¤íŠ¸ ì•ˆì— ì´ í† í°ì´ ìˆì–´ì•¼ "ì´ë¯¸ì§€ 1ì¥"ì„ ì¸ì‹í•©ë‹ˆë‹¤.
#     prompt_with_image = f"<image>\n{prompt}"
    
#     # 2) processorë¡œ ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
#     #    LLaVAì˜ AutoProcessorê°€ 'pixel_values'ì™€ 'input_ids' ë“±ì„ êµ¬ì„±í•´ì¤Œ
#     inputs = processor(
#         text=prompt_with_image,
#         images=image,
#         return_tensors="pt"
#     )

#     # 3) deviceë¡œ ì´ë™
#     inputs = {k: v.to(device) for k, v in inputs.items()}
    
#     # 4) ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
#     generation_kwargs = dict(
#         max_new_tokens=256,
#         do_sample=False,
#         temperature=0.1,
#         top_p=0.95
#     )

#     # 5) ì¶”ë¡  ì‹¤í–‰
#     with torch.no_grad():
#         output_ids = model.generate(**inputs, **generation_kwargs)

#     # 6) ê²°ê³¼ ë””ì½”ë”©
#     #    processor.decode(...)ë¡œ IDë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜.
#     #    (LLaVAëŠ” ëŒ€ì²´ë¡œ "prompt + ë‹µë³€" í˜•íƒœë¡œ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
#     generated_text = processor.decode(output_ids[0], skip_special_tokens=True)
    
#     print("Raw generated_text:", repr(generated_text))

#     # 7) promptê°€ ê²°ê³¼ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œê±°(í•„ìš”í•œ ê²½ìš°)
#     #    ê°„í˜¹ ëª¨ë¸ì´ "prompt + ë‹µë³€" í†µì§¸ë¡œ ì¶œë ¥í•˜ê¸°ë„ í•˜ë¯€ë¡œ ì²˜ë¦¬
#     if prompt in generated_text:
#         answer = generated_text.split(prompt, 1)[-1].strip()
#     else:
#         answer = generated_text.strip()

#     return answer

# if __name__ == "__main__":
#     # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ (URL í˜¹ì€ ë¡œì»¬ íŒŒì¼)
#     image_path = "test.jpeg"  
#     # image_path = "https://example.com/image.jpg"

#     # ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸
#     prompt = "What do you see in this image? Please elaborate."

#     # ì¶”ë¡  ì‹¤í–‰
#     answer = generate_answer(image_path, prompt)
#     print("ëª¨ë¸ ì‘ë‹µ:", answer)
